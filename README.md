# Tutorial on Large Language Model Enhanced Reinforcement Learning for Wireless Networks
Datasets and Code Repositories for Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks
<div align="center">

## üîç Summary of Recent Projects on LLM-enhanced RL

<details>
<summary>üìä View Complete Table (Click to Expand)</summary>

# Summary of Recent Projects on LLM-enhanced RL

| Reference | GitHub Link | Tasks Supported | LLM Type | Training Data | Performance Metrics | Last Update |
|-----------|-------------|-----------------|----------|---------------|---------------------|-------------|
| [Yan et al.](https://ieeexplore.ieee.org/abstract/document/10876191) | [V2I Project](https://patrickyanz.github.io/research/topic2) | Joint V2I base-station selection (RF+THz) and autonomous driving (lane/speed) | Llama-3.1-8B, Llama-3.1-70B, ChatGPT-3.5 | On-policy simulation: 3 km 4-lane highway; 21 AVs; 5 RF BS + 20 THz BS; LLM in-context examples via distance-based retrieval | AD reward, V2I reward (weighted rate & HO), collision rate, handover probability; convergence speed | Feb. 2025 |
| [Wang et al.](https://dl.acm.org/doi/abs/10.1145/3712678.3721880) | [RL4BandwidthEstimationChallenge](https://github.com/microsoft/RL4BandwidthEstimationChallenge?tab=readme-ov-file) | Bandwidth estimation for RTC (offline RL enhanced by LLM) | GPT2-smallest, T5-base, Qwen1.5-0.5B | Real-world RTC traces with objective audio/video QoE labels from the MMSys'24 Microsoft Teams dataset | QoE improvement, throughput accuracy, latency/jitter/packet loss; robustness/generalization across unseen traces | May. 2025 |
| [Yan et al.](https://ieeexplore.ieee.org/abstract/document/10979848) | [MATD3VVC_LLM](https://github.com/BruceCheng24/MATD3VVC_LLM) | Distribution Volt/VAR control (regional, multi-agent) on IEEE 33/123-bus | GPT-4 (zero-shot; no fine-tuning) | LLM-generated renewable operation datasets (PV/WT/ES; 3√ó100-day scenarios) for MATD3 training; modified IEEE 33/123-bus simulators | Voltage deviation, voltage violation rate, network loss, decision time, cumulative reward | Apr. 2025 |
| [Kwon et al.](https://openreview.net/forum?id=10uNUgI5Kl) | [reward_design_with_llms](https://github.com/minaek/reward_design_with_llms) | Reward design in RL (Ultimatum, Matrix Games, DealOrNoDeal) | GPT-3 | Few/zero-shot NL prompts; LLM labels on-policy rollouts; 10-user pilot study | Labeling accuracy; user alignment score; episode return/win rate | May. 2023 |
| [Shi et al.](https://openreview.net/forum?id=AY6aM13gGF) | [LaMo-2023](https://github.com/srzer/LaMo-2023) | Offline RL (D4RL MuJoCo, Kitchen, Atari) | GPT-2 (gpt2 / gpt2-medium) | Pretrained on WikiText-103; offline datasets: D4RL (Hopper, Walker2d, HalfCheetah, Reacher2d), Kitchen, d4rl-atari (Breakout/Q*bert/Pong) | Normalized return/score (D4RL, Atari); sample efficiency in low-data regime; sparse-reward performance vs CQL/IQL/TD3+BC | Dec. 2024 |
| [Shinn et al.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html) | [reflexion](https://github.com/noahshinn/reflexion) | Decision-making (ALFWorld), reasoning (HotPotQA), programming (HumanEval, MBPP) | GPT-3/3.5, GPT-4 | ALFWorld; HotPotQA; HumanEval; MBPP; LeetcodeHardGym | Success rate (ALFWorld); Exact Match (HotPotQA); Pass@1 (code) | Jan. 2025 |
| [Paischer et al.](https://proceedings.mlr.press/v162/paischer22a.html) | [helm](https://github.com/ml-jku/helm) | Partially observable RL (history compression); MiniGrid memory tasks, Procgen (memory mode), RandomMaze | Transformer-XL (257M, pretrained on WikiText-103; frozen) | Pretrained TrXL on WikiText-103; on-policy PPO rollouts; FrozenHopfield maps observations to token embeddings; no LM finetuning | Episodic return; sample efficiency (env steps to threshold); SOTA on MiniGrid/Procgen memory tasks | Nov. 2024 |
| [Dalal et al.](https://openreview.net/forum?id=hQVCCxQrYN) | [planseqlearn](https://github.com/mihdalal/planseqlearn) | Long-horizon robot control (multi-stage manipulation; vision-based) | GPT-4 (planner; GPT-4V used to pre-generate plans) | Meta-World, ObstructedSuite, Kitchen, Robosuite (MuJoCo; online interaction) | Task success rate; sample efficiency (success vs trials); robustness to pose noise | May. 2024 |
| [Xie et al.](https://openreview.net/forum?id=tUM39YTRxH) | [text2reward](https://github.com/xlang-ai/text2reward) | Reward shaping for RL (robotic manipulation, locomotion); real-robot deployment | GPT-4; ChatGPT; Llama-2-7B-Chat; CodeLlama-34B-Instruct | ManiSkill2, MetaWorld, Gym MuJoCo & data-free goal descriptions with compact env abstractions; optional human feedback | Task success rate; training convergence speed; human-evaluated rollout success; real-robot success | May. 2024 |
| [Ma et al.](https://openreview.net/forum?id=IEduRUO55F) | [Eureka](https://github.com/eureka-research/Eureka) | Reward design across 29 RL envs (10 robot morphologies; dexterous manipulation incl. pen-spinning; locomotion) | GPT-4; GPT-3.5 (coding LLMs) | Env source code + NL task descriptions; Isaac Gym rollouts (29 open-source envs); optional human preference feedback | Normalized return & task success; human reward tasks; avg normalized improvement | May. 2024 |
| [Zhou et al.](https://dl.acm.org/doi/abs/10.24963/ijcai.2024/627) | [LLM4Teach](https://github.com/ZJLAB-AMMI/LLM4Teach) | Embodied decision making: MiniGrid (e.g., DoorKey/multi-room) and Habitat navigation | Vicuna-7B, GLM-130B as LLM teachers; lightweight RL student (24K‚Äì10M params) | On-policy rollouts in MiniGrid/Habitat; library of option policies; uncertainty-aware soft instructions from LLM for policy distillation | Success rate; sample efficiency (env steps to threshold); improved accuracy vs. LLM-only with far lower compute; student surpasses teacher | May. 2024 |
| [Chen et al.](https://ieeexplore.ieee.org/abstract/document/10529514) | [rlingua](https://rlingua.github.io/) | Robotic manipulation (panda_gym; RLBench: 12 sparse-reward tasks; sim2real pick-and-place) | ChatGPT (OpenAI; coding LLM) | LLM-generated rule-based controller via prompt engineering; on-policy TD3 rollouts in panda_gym/RLBench; real-robot evaluation via sim2real transfer | Task success rate; sample efficiency (steps to threshold); convergence speed; sim2real success vs. TD3 baseline | Jul. 2024 |
| [Li et al.](https://www.sciencedirect.com/science/article/abs/pii/S0925231225007775) | [pymarl_LLM](https://github.com/lizhemin18/pymarl_LLM) | Multi-agent cooperative control in SMAC; LLM-guided decision-making toolkit | Model-agnostic (pluggable open-source LLMs) | On-policy SMAC rollouts with QMIX baseline; SC2 v2.4.10; automatic prompt generation and strategy conversion | Win rate (SMAC), steps-to-threshold / convergence speed; ablations with/without LLM guidance | Jul. 2025 |
| [Carta et al.](https://proceedings.mlr.press/v202/carta23a.html) | [Grounding_LLMs_with_online_RL](https://github.com/flowersteam/Grounding_LLMs_with_online_RL) | Text-based RL (BabyAI-Text): spatial navigation & instruction following | Flan-T5 (Small 80M, Large 780M, XL 3B) | On-policy online rollouts in BabyAI-Text; PPO fine-tuning of LLM with value head; BC from expert bot for comparison | Success rate; sample efficiency (SE); generalization to OOV/synonyms/French; robustness to distractors/action-space changes | Oct. 2024 |
| [Rocamonde et al.](https://openreview.net/forum?id=N0I2RtD8je) | [vlmrm](https://github.com/AlignmentResearch/vlmrm) | Vision-based control: MuJoCo Humanoid pose tasks; classic control (CartPole, MountainCar) | CLIP VLMs (open_clip; e.g., ViT-g/14 LAION2B); scaling across CLIP sizes | Environment rollouts with zero-shot CLIP rewards from NL prompts (goal+baseline); no reward labels; SAC/DQN (SB3) | Human success rate (humanoid); EPIC distance to GT/human labels; policy performance under GT rewards; scaling trends | Mar. 2024 |
| ‚Äî | [MARTI](https://github.com/TsinghuaC3I/MARTI) | LLM-based multi-agent RL (debate/chain-of-agents/MoA; async workflows) | Model-agnostic; multi-agent | Math/code/search tasks; centralized reward shaping | AIME accuracy; multi-agent vs single improvements | Oct. 2025 |

</details>

</div>
